# -*- coding: utf-8 -*-
"""Final_LabTest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ce2yUM47Zh_s6_9pOUyMWKi9rDeBsRRo
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import time

# Step 1: Load the dataset
data = pd.read_csv('/content/dataset.csv')

# Step 2: Extract information
# Number of instances
num_instances = data.shape[0]

# Number of features
num_features = data.shape[1] - 1  # Excluding the target column

# Number of instances from each class (malware/benign)
class_counts = data['classification'].value_counts()

# Step 3: Perform EDA (Exploratory Data Analysis)
# For example:
print("\nFirst few rows of the dataset:")
print(data.head())

# Statistical summary of numerical columns
print("\nSummary statistics of numerical columns:")
print(data.describe())

# Information about the dataset
print("\nInformation about the dataset:")
print(data.info())

# Step 5: Split the dataset into training and testing sets
X = data.drop('classification', axis=1)  # Features
y = data['classification']  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Create machine learning classifiers
nb_classifier = GaussianNB()
svm_classifier = SVC()
knn_classifier = KNeighborsClassifier()

# Step 7: Train the classifiers
nb_classifier.fit(X_train, y_train)
svm_start_time = time.time()
svm_classifier.fit(X_train, y_train)
svm_end_time = time.time()
knn_start_time = time.time()
knn_classifier.fit(X_train, y_train)
knn_end_time = time.time()

# Step 8: Test the classifiers and generate confusion matrices
classifiers = [nb_classifier, svm_classifier, knn_classifier]
classifier_names = ['Naive Bayes', 'SVM', 'KNN']

for classifier, name in zip(classifiers, classifier_names):
    start_time = time.time()
    y_pred = classifier.predict(X_test)
    end_time = time.time()

    processing_time = end_time - start_time
    print(f"\n{classifier.__class__.__name__} processing time: {processing_time:.2f} seconds")

    cm = confusion_matrix(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nConfusion Matrix - {name}:")
    print(cm)
    print(f"\nAccuracy - {name}: {acc:.2f}")

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {name}\nAccuracy: {acc:.2f}')
    plt.show()

# Step 9: Compare the processing time among the models and discuss your recommendation
nb_train_time = end_time - start_time  # Time taken to train Naive Bayes classifier
svm_train_time = svm_end_time - svm_start_time  # Time taken to train SVM classifier
knn_train_time = knn_end_time - knn_start_time  # Time taken to train KNN classifier

print("\nProcessing time comparison:")
print(f"Naive Bayes training time: {nb_train_time:.2f} seconds")
print(f"SVM training time: {svm_train_time:.2f} seconds")
print(f"KNN training time: {knn_train_time:.2f} seconds")

# Based on the results, discuss recommendations for model selection, considering both performance and efficiency
print("\nRecommendations:")
print("Based on the accuracy and processing time, the Naive Bayes classifier shows promising results.")
print("However, if the dataset size increases significantly, the SVM classifier might be a better choice due to its efficient use of memory.")